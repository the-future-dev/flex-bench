\section{Introduction}

Since the publication of RT-1 \cite{RT-1}, the robotics research is undergoing a prolific period focused around foundation models. The central promise of this paradigm is the development of generalist policies that aim to achieve general-purpose embodied intelligence, capable of complex interactions within unstructured environments. This shift mirrors advancements in Machine Learning, particularly in Natural Language Processing and Computer Vision, where large pretrained models have demonstrated remarkable success.

\begin{wrapfigure}{r}{0.5\textwidth}
    \vspace{-15pt}
    \centering
    \includegraphics[width=0.45\textwidth]{images/vla.jpg}
    \caption{VLA abstraction}
    \label{fig:vla_abstraction}
    \vspace{-15pt}
    % https://docs.google.com/presentation/d/1F3-KIjQxwBtXfJAlc9_vwuBLSGuZqcXqZ7_GfxX5kYY/edit?usp=sharing
\end{wrapfigure}

Robotics foundational models, often referred to as Vision-Language-Action (VLA) models, are the policies 
emerging from this paradigm shift. This nomenclature reflects their modality: vision and language serve as inputs while 
actions serve as outputs (Figure \ref{fig:vla_abstraction}). Leveraging techniques such as Imitation Learning and 
Reinforcement Learning, these models have demonstrated remarkable capabilities in dexterous manipulation across multiple tasks, 
offering a degree of generalization over robot embodiments and environments. The key enabler of this progress is the 
usage of a pretrained vision language model for initialization of the model (or part of the model), which then 
exploits a vast and varied robotics dataset to train the Vision Language Action Model.

\section{Goals and Objectives}
Due to the rapid pace of development in robotics foundation models, the field currently lacks established, comprehensive comparison benchmarks,
particularly for complex interaction scenarios like deformable object manipulation (DOM).
This absence makes rigorous comparison between foundational models challenging, while simultaneously presenting an opportunity for standardization.
Although some models like $\pi_0$ \cite{pi_zero} have demonstrated capabilities in specific DOM tasks (Shirt/Towel/Laundry Folding), a standardized evaluation suite is still missing.
Such a suite is essential for equitable evaluation and deeper understanding of model capabilities across different manipulation scenarios.

\subsection{Overall Goal}
The primary objective of this project is to develop DOM-Bench: a standardized benchmarking suite tailored for evaluating robotics foundation models on deformable object manipulation tasks. This will provide a coherent and equitable basis for comparing model capabilities and limitations.

This project aims to deliver:
\begin{itemize}
    \item DOM-Bench Suite: A set of deformable object manipulation tasks, environments, and metrics designed to evaluate robotics foundation models.
    \item Evaluation Protocol and Implementation: Specify the procedure for applying the benchmark suite to different models, e.g: specifications for data efficiency testing and implement the protocol on selected models.
    \item Paper: Summarization of findings, and dissemination through a research paper.
\end{itemize}


\subsection{Specific Objectives}
\vspace{4pt}
% To achieve the overall goal, the following specific objectives are defined:

\subsubsection{Literature Review} % Using subsubsection for better structure
The first task is to conduct a comprehensive review covering: existing robotics benchmarks, evaluation methodologies, and taxonomies, with a focus on deformable object manipulation and metrics used; Robotics foundation models (VLAs), detailing architectures, training data/recipes, and claimed capabilities relevant to manipulation.

\subsubsection{Benchmark Design (DOM-Bench)}
Following our review, we will propose DOM-Bench, a benchmark suite and evaluation protocol designed for evaluating robotics foundational models over Deformable Object Manipulation (DOM) tasks. The primary goal of DOM-Bench is to offer deeper insights into a model's capabilities, moving beyond simple task success rates.
To achieve this, DOM-Bench will employ a multi-tier task structure. Tasks are organiyzed into tiers of increasing complexity, with each tier designed to evaluate distinct capabilities using specific, measurable metrics. (A detailed brainstorming of potential tasks can be found in Appendix~\ref{app:task_suite}).
% \begin{enumerate}
%     \item \textit{Fundamentals}: the focus of the first tier is evaluating the shape control and manipulation primitives using classical objects. Classical examples can be Cloth Folding or Rope Straightening.	
%     \item \textit{Complex Interactions}: the aim on this second type of tasks is to test the model capabilities on more sophisticated interaction, multi-step execution, or tool use. 
%     \item \textit{Foundation Model Evaluation}: the final layer of tasks points to evaluating language understanding, reasoning, planning, and generalization capabilities inherent to foundation models when applied to DOM.
% \end{enumerate}

\paragraph{\textbf{Primitive DOM manipulation} (Minimum Requirement)}

The first result is to gather performance over a series of simple tasks (Appendix~\ref{app:task_suite_1}) whose aim is to evaluate the core deformable object manipulation abilities of the models.

\begin{domexample}
Picking a task for each type of deformable object (rope 1D, fabric 2D and liquid 3D) from the DaXBench simulator - \{WhipRope, FoldTShirt, PourSoup\}. This test will cover a single environment, and a single embodiment (since the DaXBench provides end effector control), therefore we will need to generate forward and inverse kinematics for coherence with the output of the models.
\end{domexample}

\paragraph{\textbf{Evaluating Data Efficiency} (Minimum Requirement) }

Beyond task performance, a key aspect of DOM-Bench is the measurement of data efficiency. We aim to quantify how model performance changes based on the amount of fine-tuning data provided. Specifically, we plan to record performance under different conditions: zero-shot (no fine-tuning examples), fine-tuned with 10 examples, fine-tuned with 30 examples.

\begin{domexample}
Consider evaluating a Vision-Language-Action (VLA) model on a set of tasks: \{HangClothes, StoreInCloset, PourRice\}. Where HangClothes and StoreInCloset are defined in GarmentLab (a simulated environment, with a specific embodiment), while PourRice will be executed in the RPL lab (a real environment, where we can test different embodiment, e.g., SO-100 and +1 from RPL). To measure data efficiency comprehensively, we need to aggregate performance across these variations. For a single VLA model, the total number of tests required would be calculated based on the variations per task: 1 (HangClothes) + 1 (StoreInCloset) + 2 (PourRice, one for embodiment) = 4 variations. And finally each variation will be tested under 3 different data conditions \{ \text{0-shot, 10 demos, 30 demos} \} resulting in 12 tests.
\end{domexample}

% \textbf{Total Variations:} 1 (HangClothes) + 1 (StoreInCloset) + 2 (PourRice, one for embodiment) = 4 variations. Considering that each variation needs to be tested under the 3 different data conditions: \{ \text{0-shot, 10 demos, 30 demos} \} the first metric needs 12 tests

% For instance, considering the full scope mentioned in the reference might involve 3 tasks, 2 simulation environments, 1 real environment, and 3 robot embodiments, leading to a more extensive set of tests.

This approach allows for measuring generalization across tasks, environments (simulation and real), and robot embodiments, similar to the methodology in \cite{TransferWelle}.

\paragraph{\textbf{Further Tasks Expansion} (Stretch Goal)} 
Include more tasks for previous evaluation metrics.

\paragraph{\textbf{Long-Horizon Planning Tasks} (Stretch Goal)} 
Design specific long-horizon tasks (picking from Appendix~\ref{app:task_suite_3}) to quantitatively assess the effective context length a model can utilize for successful task completion with DOM. This involves evaluating performance degradation as task complexity and required memory increase.

\paragraph{\textbf{Instruction Following Evaluation} (Stretch Goal)} 
Develop tasks that require precise instruction following to evaluate the model's ability to understand and execute complex language-conditioned tasks. This includes tasks with specific spatial, relational, and attribute-based instructions.

\paragraph{\textbf{Vocabulary Generalization} (Stretch Goal)} Create tasks that test the model's ability to generalize to new vocabulary, including novel objects and actions not seen during training. This involves evaluating the model's robustness and flexibility in handling diverse and complex instructions.

\paragraph{\textbf{Explainable AI Integration} (Stretch Goal) } Incorporate methods (e.g., representational probing \cite{Probing-VLA}, attention analysis) to gain insights into the models' internal representations and decision-making processes during DOM tasks. This aims to move beyond black-box evaluation towards understanding \textit{why} models succeed or fail.

% \paragraph{Protocol Definition:} Define a precise experimental protocol specifying setup, initial state randomization, number of trials, permissible interventions (if any), and data logging requirements. Reference Appendix \ref{app:task_suite} for detailed task specifications.

\subsubsection{Evaluation \& Analysis}
Finally we will implement the DOM-Bench evaluation protocol across selected foundation models.
We will initially focus on a core set of publicly available models, $\pi_0$ \cite{pi_zero}, $\text{OpenVLA}$ \cite{OpenVLA}, and $\text{Gr00t}$ \cite{Gr00tN1}, and if time allows, we will expand the evaluation to include additional models, $\text{OpenVLA-OFT}$ \cite{OpenVLA-OFT} and $\text{Octo}$ \cite{Octo}.
This comprehensive evaluation will establish baseline performance metrics for future research and
highlight specific areas where current foundation models excel or require improvement.

\paragraph{Stretch Goals:}

Evaluation expansion can be achieved by adding additional models such as: $\text{OpenVLA-OFT}$ \cite{OpenVLA-OFT} and $\text{Octo}$ \cite{Octo}.

